<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
  /*
   * Copyright 2013 Christophe-Marie Duquesne <chmd@chmd.fr>
   *
   * CSS for making a resume with pandoc. Inspired by moderncv.
   *
   * This CSS document is delivered to you under the CC BY-SA 3.0 License.
   * https://creativecommons.org/licenses/by-sa/3.0/deed.en_US
   */
  
  /* Whole document */
  body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      width: 800px;
      margin: auto;
      background: #FFFFFF;
      padding: 10px 10px 10px 10px;
  }
  
  /* Title of the resume */
  h1 {
      font-size: 55px;
      color: #757575;
      text-align:center;
      margin-bottom:15px;
  }
  h1:hover {
      background-color: #757575;
      color: #FFFFFF;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Titles of categories */
  h2 {
      /* This is called "sectioncolor" in the ConTeXt stylesheet. */
      color: #397249;
  }
  /* There is a bar just before each category */
  h2:before {
      content: "";
      display: inline-block;
      margin-right:1%;
      width: 16%;
      height: 10px;
      /* This is called "rulecolor" in the ConTeXt stylesheet. */
      background-color: #9CB770;
  }
  h2:hover {
      background-color: #397249;
      color: #FFFFFF;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Definitions */
  dt {
      float: left;
      clear: left;
      width: 17%;
      /*font-weight: bold;*/
  }
  dd {
      margin-left: 17%;
  }
  p {
      margin-top:0;
      margin-bottom:7px;
  }
  
  /* Blockquotes */
  blockquote {
      text-align: center
  }
  
  /* Links */
  a {
      text-decoration: none;
      color: #397249;
  }
  a:hover, a:active {
      background-color: #397249;
      color: #FFFFFF;
      text-decoration: none;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Horizontal separators */
  hr {
      color: #A6A6A6;
  }
  
  table {
      width: 100%;
  }
  </style>
</head>
<body>
<h1 id="cs583-deep-learning">CS583: Deep Learning</h1>
<blockquote>
<p>Instructor: Shusen Wang</p>
</blockquote>
<h2 id="read-this-before-taking-the-course">Read this before taking the course!</h2>
<p>The course requires very heavy workload (in fact much heavier than the previous semester). <span style="color:red"><strong>Do NOT take this course unless you can spend much time on this course.</strong></span></p>
<ul>
<li><p>You are required (instead of &quot;recommended&quot;) to read the text book, &quot;Deep learning with Python&quot;, and run the example code. In the quizzes and final exam, there will be questions based on the book's content, especially the content not covered in the class.</p></li>
<li><p>You are required to participate a Kaggle competition and get a decent score and ranking.</p></li>
<li><p>There are at least 5 homework.</p></li>
<li><p>There are additional programming assignment, by doing which you will receive bonus score. Get &quot;A&quot; is difficult without collecting the bonus.</p></li>
</ul>
<p><span style="color:red"><strong>Be serious about the prerequisites. Do NOT take the course if you do not meet the prerequisite requirements.</strong></span></p>
<ul>
<li><p>In the 2nd class, you will be asked to do a quiz (weight in the grading: around 5%). The quiz will test your knowledge in elementary algebra and calculus and Python programming (especially NumPy).</p></li>
<li><p>There will be a quiz on matrix algebra, matrix calculus, and optimization.</p></li>
<li><p>In the final example, there will be questions on matrix algebra, differentiation, optimization, and Python code understanding.</p></li>
</ul>
<h2 id="description">Description</h2>
<p><strong>Meeting Time:</strong></p>
<ul>
<li>TBD</li>
</ul>
<p><strong>Office Hours:</strong></p>
<ul>
<li>Thursday, 3:00 - 5:00 PM, North Building 205</li>
</ul>
<p><strong>Contact the Instructor:</strong></p>
<ul>
<li><p>For questions regarding grading, talk to the instructor during office hours or send him emails.</p></li>
<li><p>For any other questions, come during the office hours; the instructor will NOT reply such emails.</p></li>
</ul>
<p><strong>Prerequisite:</strong></p>
<ul>
<li><p>Elementary linear algebra, e.g., matrix multiplication, eigenvalue decomposition, and matrix norms.</p></li>
<li><p>Elementary calculus, e.g., convex function, differentiation of scalar functions, first derivative, and second derivative.</p></li>
<li><p>Python programming (especially the Numpy library) and Jupyter Notebook.</p></li>
</ul>
<p><strong>Goal:</strong> This is a practical course; the students will be able to use DL methods for solving real-world ML, CV, and NLP problems. The students will also learn math and theories for understanding ML and DL.</p>
<h2 id="schedule">Schedule</h2>
<p>TBD</p>
<h2 id="syllabus-and-slides">Syllabus and Slides</h2>
<ol style="list-style-type: decimal">
<li><p><strong>Machine learning basics.</strong> This part briefly introduces the fundamental ML problems-- regression, classification, dimensionality reduction, and clustering-- and the traditional ML models and numerical algorithms for solving the problems.</p>
<ul>
<li><p>ML basics. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/1_ML_Basics.pdf">slides</a>]</p></li>
<li><p>Regression. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/2_Regression_1.pdf">slides-1</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/2_Regression_2.pdf">slides-2</a>]</p></li>
<li><p>Classification. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_1.pdf">slides-1</a>][<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_2.pdf">slides-2</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_3.pdf">slides-3</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_4.pdf">slides-4</a>]</p></li>
<li><p>Regularizations. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Optimization.pdf">slides-1</a>][<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Regularizations.pdf">slides-2</a>]</p></li>
<li><p>Clustering. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/4_Clustering.pdf">slides</a>]</p></li>
<li><p>Dimensionality reduction. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/5_DR_1.pdf">slides-1</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/5_DR_2.pdf">slides-2</a>]</p></li>
<li><p>Scientific computing libraries. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/5_DR_3.pdf">slides</a>]</p></li>
</ul></li>
<li><p><strong>Neural network basics.</strong> This part covers the multilayer perceptron, backpropagation, and deep learning libraries, with focus on Keras.</p>
<ul>
<li><p>Multilayer perceptron and backpropagation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/6_NeuralNet_1.pdf">slides</a>]</p></li>
<li><p>Keras. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/6_NeuralNet_2.pdf">slides</a>]</p></li>
<li><p>Further reading: [<a href="https://adl1995.github.io/an-overview-of-activation-functions-used-in-neural-networks.html">activation functions</a>][<a href="https://isaacchanghau.github.io/post/loss_functions/">loss functions</a>] [<a href="https://isaacchanghau.github.io/post/weight_initialization/">parameter initialization</a>][<a href="https://isaacchanghau.github.io/post/parameters_update/">optimization algorithms</a>]</p></li>
</ul></li>
<li><p><strong>Convolutional neural networks (CNNs).</strong> This part is focused on CNNs and its application to computer vision problems.</p>
<ul>
<li><p>CNN basics. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_1.pdf">slides-1</a>][<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_2.pdf">slides-2</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_3.pdf">slides-3</a>]</p></li>
<li><p>Advanced topics on CNNs. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_4.pdf">slides</a>]</p></li>
<li><p>Popular CNN architectures. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_5.pdf">slides</a>]</p></li>
<li><p>Face recognition. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_6.pdf">slides</a>]</p></li>
<li><p>Further reading: [style transfer (Section 8.1, Chollet's book)][visualize CNN (Section 5.4, Chollet's book)]</p></li>
</ul></li>
<li><p><strong>Autoencoders.</strong> This part introduces autoencoders for dimensionality reduction and image generation.</p>
<ul>
<li><p>Autoencoder for dimensionality reduction. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/8_AE_1.pdf">slides</a>]</p></li>
<li><p>Variational Autoencoders (VAEs) for image generation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/8_AE_2.pdf">slides</a>]</p></li>
</ul></li>
<li><p><strong>More image generation methods.</strong> <strong>(Optional, depending on the progress.)</strong> This part covers two image generation approaches in addition to VAE. The training of neural networks takes fixed images (X) as inputs and optimize w.r.t. the weights (W). Conversely, one can fix the network's weights (W) and optimize w.r.t. the input X. In this way, new images are generated to maximize (or minimize) some function; the application includes attacking neural networks and deep dream. Another very different approach is the generative adversarial network (GAN).</p>
<ul>
<li><p>Attack neural networks.</p></li>
<li><p>Deep dream.</p></li>
<li><p>Generative adversarial network (GAN).</p></li>
</ul></li>
<li><p><strong>Recurrent neural networks (RNNs).</strong> This part introduces RNNs and its applications in natural language processing (NLP).</p>
<ul>
<li><p>Text processing. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_1.pdf">slides</a>]</p></li>
<li><p>RNN basics and LSTM. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_2.pdf">slides</a>][<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">reference</a>]</p></li>
<li><p>Text generation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_3.pdf">slides</a>]</p></li>
<li><p>Machine translation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_4.pdf">slides</a>]</p></li>
<li><p>Attention. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_5.pdf">slides</a>][<a href="https://distill.pub/2016/augmented-rnns/">reference-1</a>] [<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">reference-2</a>]</p></li>
<li><p>Further reading: [<a href="http://www.aclweb.org/anthology/D14-1162">GloVe: Global Vectors for Word Representation</a>][<a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>]</p></li>
</ul></li>
<li><p><strong>Recommender system.</strong> <strong>(Optional, depending on the progress.)</strong> This part is focused on the collaborative filtering approach to recommendation based on the user-item rating data. This part covers matrix completion methods and neural network approaches.</p>
<ul>
<li>Collaborative filtering. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/11_Recommender.pdf">slides</a>]</li>
</ul></li>
</ol>
<h2 id="project">Project</h2>
<p>Every student must participate in one <a href="https://www.kaggle.com/competitions">Kaggle competition</a>.</p>
<ul>
<li><p><strong>Details</strong>: [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/project/Project/proj.pdf">click here</a>]</p></li>
<li><p><strong>Teamwork policy</strong>: You had better work on your own project. Teamwork (up to 3 students) is allowed if the competition has a heavy workload; the workload and team size will be considered in the grading.</p></li>
<li><p><strong>Grading policy</strong>: see the tentative evaluation form [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/project/Evaluation/Evaluation.pdf">click here</a>]</p></li>
</ul>
<h2 id="textbooks">Textbooks</h2>
<p><strong>Required</strong> (Please notice the difference between &quot;required&quot; and &quot;recommended&quot;):</p>
<ul>
<li>Francois Chollet. Deep learning with Python. Manning Publications Co., 2017. (Available online.)</li>
</ul>
<p><strong>Recommended</strong>:</p>
<ul>
<li><p>Y. Nesterov. Introductory Lectures on Convex Optimization Book. Springer, 2013. (Available online.)</p></li>
<li><p>D. S. Watkins. Fundamentals of Matrix Computations. John Wiley &amp; Sons, 2004.</p></li>
<li><p>I. Goodfellow, Y. Bengio, A. Courville, Y. Bengio. Deep learning. MIT press, 2016. (Available online.)</p></li>
<li><p>M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2012.</p></li>
<li><p>J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning. Springer series in statistics, 2001. (Available online.)</p></li>
</ul>
<h2 id="grading-policy">Grading Policy</h2>
<p><strong>Grading percentages</strong>:</p>
<ul>
<li><p>Homework 45%</p></li>
<li><p>Final 15%</p></li>
<li><p>Project 20%</p></li>
<li><p>Quizzes 20%</p></li>
<li><p>Bonus (up to 10%)</p></li>
</ul>
<p><strong>Late penalty</strong>:</p>
<ul>
<li><p>Late submissions of assignments or project document for whatever reason will be punished. 1% of the score of an assignment/project will be deducted per day. For example, if an assignment is submitted 15 days and 1 minute later than the deadline (counted as 16 days) and it gets a grade of 95%, then the score after the deduction will be: 95% - 16% = 79%.</p></li>
<li><p>Dec 20 is the firm deadline for all the homework and the course project. Submissions later than the firm deadline will not be graded.</p></li>
</ul>
</body>
</html>
